{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1bf931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 1011.15 MB\n",
      "MongoDB connected successfully\n",
      "Memory usage: 1013.69 MB\n",
      "Memory usage: 1013.70 MB\n",
      "Memory usage: 1013.73 MB\n",
      "MongoDB Feedback: 68 questions, Labels: {0, 1, 2}\n",
      "Memory usage: 1018.11 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "Memory usage: 992.72 MB\n",
      "Memory usage: 992.73 MB\n",
      "Memory usage: 992.73 MB\n",
      "Memory usage: 992.75 MB\n",
      "Total training examples: 249\n",
      "Greeting examples: 38\n",
      "File-related examples: 128\n",
      "General examples: 84\n",
      "Unique labels: {0, 1, 2}\n",
      "Memory usage: 992.78 MB\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 263\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m    262\u001b[39m     indices = [i \u001b[38;5;28;01mfor\u001b[39;00m i, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_labels) \u001b[38;5;28;01mif\u001b[39;00m l == label]\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     questions = [\u001b[43mall_questions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[32m    264\u001b[39m     labels = [all_labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[32m    265\u001b[39m     repeat_factor = max_count // \u001b[38;5;28mlen\u001b[39m(questions) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(questions) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import motor.motor_asyncio\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import psutil\n",
    "\n",
    "nest_asyncio.apply()\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 2: MongoDB Connection\n",
    "try:\n",
    "    MONGO_URI = \"mongodb://localhost:27017/csv_validation_db\"\n",
    "    client = motor.motor_asyncio.AsyncIOMotorClient(MONGO_URI)\n",
    "    db = client[\"csv_validation_db\"]\n",
    "    user_feedback_collection = db[\"user_feedback\"]\n",
    "    validation_logs_collection = db[\"validation_logs\"]\n",
    "    print(\"MongoDB connected successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"MongoDB connection failed: {e}\")\n",
    "    feedback_questions, feedback_labels, monthly_counts, file_ids = [], [], defaultdict(int), set([\"1\", \"2\", \"3\"])\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 3: Synonym Mapping\n",
    "SYNONYM_MAP = {\n",
    "    \"file\": [\"files\", \"document\", \"doc\", \"dataset\", \"spreadsheet\"],\n",
    "    \"upload\": [\"uploaded\", \"uploading\", \"submit\", \"send\", \"provide\"],\n",
    "    \"validate\": [\"check\", \"verify\", \"validate\", \"inspect\", \"examine\"],\n",
    "    \"error\": [\"errors\", \"mistake\", \"issue\", \"problem\", \"bug\", \"anomaly\", \"anomalies\"],\n",
    "    \"report\": [\"pdf\", \"document\", \"summary\", \"analysis\"],\n",
    "    \"history\": [\"log\", \"record\", \"past\", \"previous\"],\n",
    "    \"how\": [\"hw\", \"hows\", \"method\", \"way\"],\n",
    "    \"many\": [\"much\", \"number of\", \"count\", \"quantity\"],\n",
    "    \"status\": [\"state\", \"progress\", \"condition\", \"situation\"],\n",
    "    \"generate\": [\"create\", \"make\", \"produce\", \"build\"],\n",
    "    \"application\": [\"app\", \"tool\", \"software\", \"system\", \"platform\"],\n",
    "    \"show\": [\"display\", \"list\", \"present\", \"reveal\"],\n",
    "    \"id\": [\"number\", \"identifier\", \"code\"],\n",
    "}\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 4: Text Preprocessing Function\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    corrections = {\n",
    "        \"whn\": \"when\", \"wat\": \"what\", \"wen\": \"when\", \"hw\": \"how\",\n",
    "        \"pls\": \"please\", \"plz\": \"please\", \"thx\": \"thanks\",\n",
    "        \"u\": \"you\", \"r\": \"are\", \"yr\": \"your\", \"wht\": \"what\",\n",
    "        \"cn\": \"can\", \"whr\": \"where\", \"hws\": \"how's\",\n",
    "        \"errrs\": \"errors\", \"eror\": \"error\", \"mistak\": \"mistake\"\n",
    "    }\n",
    "    for wrong, right in corrections.items():\n",
    "        text = re.sub(r'\\b' + wrong + r'\\b', right, text)\n",
    "    \n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = str(blob.correct()) if len(text) < 50 else text\n",
    "    \n",
    "    number_pattern = re.compile(r'\\b(\\d+)\\b')\n",
    "    numbers = number_pattern.findall(corrected_text)\n",
    "    number_map = {num: f\"<NUM{idx}>\" for idx, num in enumerate(numbers)}\n",
    "    temp_text = corrected_text\n",
    "    for num in numbers:\n",
    "        temp_text = re.sub(r'\\b' + num + r'\\b', number_map[num], temp_text)\n",
    "    \n",
    "    words = temp_text.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word.startswith('<NUM'):\n",
    "            processed_words.append(word)\n",
    "            continue\n",
    "        word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if word:\n",
    "            replaced = False\n",
    "            for canonical, synonyms in SYNONYM_MAP.items():\n",
    "                if word in synonyms or word == canonical:\n",
    "                    processed_words.append(canonical)\n",
    "                    replaced = True\n",
    "                    break\n",
    "            if not replaced:\n",
    "                processed_words.append(word)\n",
    "    \n",
    "    processed_text = \" \".join(processed_words)\n",
    "    for num, placeholder in number_map.items():\n",
    "        processed_text = processed_text.replace(placeholder, num)\n",
    "    \n",
    "    return processed_text\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 5: Fetch Training Data\n",
    "async def fetch_training_data():\n",
    "    feedback = await user_feedback_collection.find().to_list(100)\n",
    "    feedback_questions = []\n",
    "    feedback_labels = []\n",
    "    \n",
    "    for f in feedback:\n",
    "        if f.get(\"intent\") is not None and \"I didn't get that\" not in f.get(\"response\", \"\"):\n",
    "            question = preprocess_text(f[\"question\"])\n",
    "            intent = f[\"intent\"]\n",
    "            if isinstance(intent, (int, float)) and intent in [0, 1, 2]:\n",
    "                feedback_questions.append(question)\n",
    "                feedback_labels.append(int(intent))\n",
    "    \n",
    "    logs = await validation_logs_collection.find().to_list(100)\n",
    "    monthly_counts = defaultdict(int)\n",
    "    file_ids = set()\n",
    "    for log in logs:\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(log[\"start_time\"].replace('Z', '+00:00'))\n",
    "            month_year = dt.strftime(\"%B %Y\").lower()\n",
    "            monthly_counts[month_year] += 1\n",
    "            file_ids.add(str(log[\"unique_id\"]))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return feedback_questions, feedback_labels, monthly_counts, file_ids\n",
    "\n",
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    feedback_questions, feedback_labels, monthly_counts, file_ids = loop.run_until_complete(fetch_training_data())\n",
    "    print(f\"MongoDB Feedback: {len(feedback_questions)} questions, Labels: {set(feedback_labels)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Data fetch failed: {e}\")\n",
    "    feedback_questions, feedback_labels, monthly_counts, file_ids = [], [], defaultdict(int), set([\"1\", \"2\", \"3\"])\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 6: Initialize DistilBERT Model\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "bert_model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 7: Define Base Training Data\n",
    "base_questions = [\n",
    "    # Greetings (label 1)\n",
    "    \"hi\", \"hello\", \"hey\", \"good morning\", \"good afternoon\", \"good evening\",\n",
    "    \"what's up\", \"howdy\", \"greetings\", \"hi there\", \"hello friend\", \"yo\",\n",
    "    \"how are you\", \"how's it going\", \"how are things\", \"what's new\", \"sup\",\n",
    "    \"hii\", \"helo\", \"hay\", \"gd mrng\", \"gd evng\", \"whats up\", \"how r u\",\n",
    "    \"hola\", \"good day\", \"hey there\", \"hi buddy\", \"hello there\", \"what's good\",\n",
    "    \"how you doing\", \"how's everything\", \"what's happening\",\n",
    "    \n",
    "    # File-related (label 2)\n",
    "    \"how to upload a file\", \"can i submit a csv\", \"where to upload documents\",\n",
    "    \"check my file errors\", \"validate my dataset\", \"inspect my spreadsheet\",\n",
    "    \"how many files today\", \"count of uploads this week\", \"number of documents yesterday\",\n",
    "    \"show file issues\", \"display validation problems\", \"get error details\",\n",
    "    \"generate data report\", \"create validation summary\", \"export analysis pdf\",\n",
    "    \"what's my upload status\", \"current validation state\", \"file processing progress\",\n",
    "    \"correct my file mistakes\", \"fix data errors\", \"resolve validation issues\",\n",
    "    \"list all uploaded files\", \"errors of file 6\", \"status of file 6\", \"erors of file 6\",\n",
    "    \"file 6 status\", \"check status of file 6\", \"errors of file 1\", \"status of file 2\",\n",
    "    \"file 3 errors\", \"check file 4 status\", \"validation issues file 5\",\n",
    "    \n",
    "    # General (label 0)\n",
    "    \"what does this app do\", \"purpose of this tool\", \"how this system works\",\n",
    "    \"is my data secure\", \"where is my information stored\", \"data privacy policy\",\n",
    "    \"supported file types\", \"acceptable formats\", \"what documents can i upload\",\n",
    "    \"how to start validation\", \"begin checking process\", \"initiate inspection\",\n",
    "    \"where to see results\", \"location of reports\", \"how to download outputs\",\n",
    "    \"what is this platform\", \"explain the software\", \"tool functionality\",\n",
    "    \"how does this app function\", \"what can this system do\", \"describe the platform\",\n",
    "    \"security of my data\", \"where are reports saved\", \"how to get started\",\n",
    "    \"what file formats work\", \"how to use this tool\", \"app overview\",\n",
    "    \"system requirements\", \"how to view validation results\", \"data storage info\"\n",
    "]\n",
    "base_labels = [1]*33 + [2]*33 + [0]*30\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 8: Define File-Specific Questions\n",
    "file_specific_questions = [\n",
    "    # File error questions\n",
    "    \"what are the errors in file 6\", \"show me errors for file 6\", \"display errors in document 6\",\n",
    "    \"list mistakes in file 6\", \"what went wrong with file 6\", \"errors for file id 6\",\n",
    "    \"problems with file number 6\", \"validation issues in file 6\", \"error details for file 6\",\n",
    "    \"show issues in file 6\", \"file 6 error report\", \"what errors in file 6\",\n",
    "    \"erors of file 6\", \"error of file 6\", \"file 6 errors\",\n",
    "    \n",
    "    # Status questions\n",
    "    \"status of file 6\", \"what is the status of file 6\", \"file 6 status\",\n",
    "    \"check status of file 6\", \"current state of file 6\", \"progress of file 6\",\n",
    "    \"validation status for file 6\", \"status file 6\", \"file 6 progress\",\n",
    "    \n",
    "    # General error questions\n",
    "    \"total number of errors\", \"how many errors in my file\", \"count of validation errors\",\n",
    "    \"number of mistakes found\", \"total errors in document\", \"how many issues were detected\",\n",
    "    \"error count\", \"summarize errors\", \"total anomalies in files\",\n",
    "    \n",
    "    # Specific error types\n",
    "    \"missing values in file 6\", \"duplicates in file 6\", \"format errors in file 6\",\n",
    "    \"anomalies in file 6\", \"mandatory field errors in file 6\", \"data type issues in file 6\",\n",
    "    \n",
    "    # Other file IDs\n",
    "    \"errors in file 1\", \"show errors for file 2\", \"errors of file 3\",\n",
    "    \"status of file 4\", \"problems in file 5\", \"error details for file 7\",\n",
    "    \"status of file 8\", \"errors in file 10\", \"status of file 1\", \"errors of file 2\"\n",
    "]\n",
    "file_specific_labels = [2] * len(file_specific_questions)\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 9: Define Dynamic File Questions\n",
    "file_questions = []\n",
    "for month, count in monthly_counts.items():\n",
    "    file_questions.extend([\n",
    "        f\"how many files in {month}\", f\"files uploaded in {month}\",\n",
    "        f\"file count in {month}\", f\"uploads in {month}\"\n",
    "    ])\n",
    "for file_id in list(file_ids)[:5]:\n",
    "    file_questions.extend([\n",
    "        f\"what are the errors in file {file_id}\", f\"show errors for file {file_id}\",\n",
    "        f\"errors of file {file_id}\", f\"status of file {file_id}\",\n",
    "        f\"check status of file {file_id}\"\n",
    "    ])\n",
    "file_labels = [2] * len(file_questions)\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 10: Combine and Validate Training Data\n",
    "all_questions = base_questions + file_specific_questions + file_questions + feedback_questions\n",
    "all_labels = base_labels + file_specific_labels + file_labels + feedback_labels\n",
    "\n",
    "all_labels = [int(label) for label in all_labels if label in [0, 1, 2]]\n",
    "if len(all_questions) != len(all_labels):\n",
    "    all_questions = all_questions[:len(all_labels)]\n",
    "print(f\"Total training examples: {len(all_questions)}\")\n",
    "print(f\"Greeting examples: {sum(1 for x in all_labels if x == 1)}\")\n",
    "print(f\"File-related examples: {sum(1 for x in all_labels if x == 2)}\")\n",
    "print(f\"General examples: {sum(1 for x in all_labels if x == 0)}\")\n",
    "print(f\"Unique labels: {set(all_labels)}\")\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 11: Balance and Augment Dataset\n",
    "label_counts = np.bincount(all_labels)\n",
    "max_count = 100\n",
    "balanced_questions = []\n",
    "balanced_labels = []\n",
    "\n",
    "for label in range(3):\n",
    "    indices = [i for i, l in enumerate(all_labels) if l == label]\n",
    "    questions = [all_questions[i] for i in indices]\n",
    "    labels = [all_labels[i] for i in indices]\n",
    "    repeat_factor = max_count // len(questions) if len(questions) > 0 else 1\n",
    "    balanced_questions.extend(questions * min(repeat_factor, 3))\n",
    "    balanced_labels.extend(labels * min(repeat_factor, 3))\n",
    "\n",
    "augmented_questions = balanced_questions.copy()\n",
    "augmented_labels = balanced_labels.copy()\n",
    "for q, l in zip(balanced_questions, balanced_labels):\n",
    "    if l == 2 and \"file\" in q:\n",
    "        augmented_questions.extend([q.replace(\"file\", \"document\"), q.replace(\"file\", \"dataset\")])\n",
    "        augmented_labels.extend([l, l])\n",
    "    if l == 2 and \"error\" in q:\n",
    "        augmented_questions.extend([q.replace(\"error\", \"issue\"), q.replace(\"error\", \"problem\")])\n",
    "        augmented_labels.extend([l, l])\n",
    "    if l == 2 and \"status\" in q:\n",
    "        augmented_questions.extend([q.replace(\"status\", \"state\"), q.replace(\"status\", \"progress\")])\n",
    "        augmented_labels.extend([l, l])\n",
    "    if l == 1:\n",
    "        augmented_questions.extend([q + \"!\", q.replace(\"hi\", \"hey\"), q.replace(\"hello\", \"hola\")])\n",
    "        augmented_labels.extend([l, l, l])\n",
    "    if l == 0:\n",
    "        augmented_questions.extend([q.replace(\"app\", \"tool\"), q.replace(\"system\", \"platform\")])\n",
    "        augmented_labels.extend([l, l])\n",
    "print(f\"Augmented examples: {len(augmented_questions)}\")\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 12: Prepare BERT Data\n",
    "inputs = tokenizer(augmented_questions, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids'].numpy()\n",
    "attention_mask = inputs['attention_mask'].numpy()\n",
    "labels = np.array(augmented_labels)\n",
    "print(f\"Unique labels in dataset: {set(labels)}\")\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 13: Split Data\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(labels)), \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "train_dataset = {\n",
    "    'input_ids': input_ids[train_idx],\n",
    "    'attention_mask': attention_mask[train_idx],\n",
    "    'labels': torch.tensor(labels[train_idx])\n",
    "}\n",
    "\n",
    "test_dataset = {\n",
    "    'input_ids': input_ids[test_idx],\n",
    "    'attention_mask': attention_mask[test_idx],\n",
    "    'labels': torch.tensor(labels[test_idx])\n",
    "}\n",
    "print(f\"Train labels: {set(train_dataset['labels'].numpy())}\")\n",
    "print(f\"Test labels: {set(test_dataset['labels'].numpy())}\")\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 14: Training Loop with Early Stopping\n",
    "optimizer = optim.AdamW(bert_model.parameters(), lr=2e-5)  # Reverted to stable LR\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_f1 = 0\n",
    "patience = 5  # Increased patience\n",
    "no_improve = 0\n",
    "\n",
    "batch_size = 16  # Added batching\n",
    "num_batches = len(train_dataset['input_ids']) // batch_size + 1\n",
    "\n",
    "bert_model.train()\n",
    "for epoch in range(20):  # Increased epochs\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(train_dataset['input_ids']), batch_size):\n",
    "        batch_ids = train_dataset['input_ids'][i:i+batch_size]\n",
    "        batch_mask = train_dataset['attention_mask'][i:i+batch_size]\n",
    "        batch_labels = train_dataset['labels'][i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = bert_model(\n",
    "            input_ids=torch.tensor(batch_ids).to(device),\n",
    "            attention_mask=torch.tensor(batch_mask).to(device),\n",
    "            labels=batch_labels.to(device)\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    \n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = bert_model(\n",
    "            input_ids=torch.tensor(test_dataset['input_ids']).to(device),\n",
    "            attention_mask=torch.tensor(test_dataset['attention_mask']).to(device),\n",
    "            labels=test_dataset['labels'].to(device)\n",
    "        )\n",
    "        test_loss = test_outputs.loss.item()\n",
    "        predictions = torch.argmax(test_outputs.logits, dim=1)\n",
    "        accuracy = accuracy_score(test_dataset['labels'], predictions)\n",
    "        report = classification_report(\n",
    "            test_dataset['labels'],\n",
    "            predictions,\n",
    "            labels=[0, 1, 2],\n",
    "            target_names=[\"General\", \"Greeting\", \"File-Related\"],\n",
    "            zero_division=0,\n",
    "            output_dict=True\n",
    "        )\n",
    "        avg_f1 = (report[\"General\"][\"f1-score\"] + report[\"Greeting\"][\"f1-score\"] + report[\"File-Related\"][\"f1-score\"]) / 3\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Average F1: {avg_f1:.4f}\")\n",
    "    print(classification_report(\n",
    "        test_dataset['labels'],\n",
    "        predictions,\n",
    "        labels=[0, 1, 2],\n",
    "        target_names=[\"General\", \"Greeting\", \"File-Related\"],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        no_improve = 0\n",
    "        torch.save(bert_model.state_dict(), \"best_bert_intent_model.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    bert_model.train()\n",
    "    print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 15: Test Model Locally\n",
    "bert_model.load_state_dict(torch.load(\"best_bert_intent_model.pt\"))\n",
    "bert_model.eval()\n",
    "test_queries = [\n",
    "    \"errors of file 6\", \"status of file 6\", \"hello\", \"what does this app do\",\n",
    "    \"erors of file 6\", \"file 6 status\", \"hi\", \"how this system works\",\n",
    "    \"hey\", \"validate my dataset\", \"is my data secure\"\n",
    "]\n",
    "for query in test_queries:\n",
    "    inputs = tokenizer(preprocess_text(query), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(\n",
    "            input_ids=inputs['input_ids'].to(device),\n",
    "            attention_mask=inputs['attention_mask'].to(device)\n",
    "        )\n",
    "    intent = torch.argmax(outputs.logits, dim=1).item()\n",
    "    intent_name = [\"General\", \"Greeting\", \"File-Related\"][intent]\n",
    "    print(f\"Query: {query} -> Intent: {intent_name} (Label: {intent})\")\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Cell 16: Save Model\n",
    "print(\"XGBoost training skipped; using DistilBERT model only.\")\n",
    "torch.save(bert_model.state_dict(), \"bert_intent_model.pt\")\n",
    "print(\"Model saved successfully:\")\n",
    "print(f\"- DistilBERT model: bert_intent_model.pt ({os.path.getsize('bert_intent_model.pt')/1024:.1f} KB)\")\n",
    "print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
